{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffd28fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## Required Packages ##############################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "\n",
    "#Read CSV file\n",
    "data = pd.read_csv('reviews.csv',low_memory = False)\n",
    "data = data.drop(columns=['id', 'name', 'asins', 'brand','categories', 'keys', 'manufacturer'], axis=1).sample(100)\n",
    "\n",
    "# data['Text'] = \\\n",
    "data['Text'].map(lambda x: re.sub('[,\\.!?]', '', x))# Convert the titles to lowercase\n",
    "data['Text'] = \\\n",
    "data['Text'].map(lambda x: x.lower())# Print out the first rows of papers\n",
    "# data['Text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6ffc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove punctuations, newlines, special characters\n",
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
    "        sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
    "        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
    "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True)\n",
    "        yield(sent)\n",
    "\n",
    "# Function to remove stopwords \n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]\n",
    "\n",
    "\n",
    "# Adding data to a list\n",
    "data = data.Text.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "# remove stop words\n",
    "data_words = remove_stopwords(data_words)\n",
    "# print(data_words[:1][0][:30])\n",
    "\n",
    "# To build the bigram and trigram models # higher threshold fewer phrases for example cute_dog, little_cute_dog\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# Function to process words by removing stopwords, by creating Bigrams and Trigrams and lemmatization\n",
    "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    texts_out = []\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "# remove stopwords again after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
    "    return texts_out\n",
    "\n",
    "# Processed text data\n",
    "data_ready = process_words(data_words) \n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_ready)\n",
    "\n",
    "# Create Corpus:Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in data_ready]\n",
    "\n",
    "# To build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=10,\n",
    "                                           passes=10,\n",
    "                                           alpha='symmetric',\n",
    "                                           iterations=100,\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7cbff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract dominant topic and associated keywords and it's percentage\n",
    "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Percentage and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_ready)\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "print(df_dominant_topic.head(10))\n",
    "doc_lens = [len(d) for d in df_dominant_topic.Text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51da1bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to show samples of sentences that most represent a given topic\n",
    "doc_lens = [len(d) for d in df_dominant_topic.Text]\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
    "                                            axis=0) \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
    "print(sent_topics_sorteddf_mallet.head(10))\n",
    "\n",
    "# To plot the word counts and the weights of each keyword \n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "data_flat = [w for w_list in data_ready for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i , weight, counter[word]])\n",
    "\n",
    "df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97b4601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Word Count and Weights of Topic Keywords\n",
    "fig, axes = plt.subplots(2,2, figsize=(12,12), sharey=True, dpi=80)\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n",
    "    ax_twin = ax.twinx()\n",
    "    ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n",
    "    ax.set_ylabel('Word Count', color=cols[i])\n",
    "    ax_twin.set_ylim(0, 0.050); ax.set_ylim(0, 100)\n",
    "    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n",
    "    ax.tick_params(axis='y', left=False)\n",
    "    ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=45, horizontalalignment= 'right')\n",
    "    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n",
    "\n",
    "fig.tight_layout(w_pad=2)    \n",
    "fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fecc599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot word clouds of Top 10 keywords in each topic\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords=stop_words,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e53fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For analyzing LDA model results using pyLDAvis package\n",
    "pyLDAvis.enable_notebook()\n",
    "p = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bdc901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To evaluate LDA model by computing Parameters, log likelihood score and model perplexity\n",
    "#Vectorize data\n",
    "vectorizer = CountVectorizer(analyzer='word',lowercase=True)\n",
    "data_vectorized = vectorizer.fit_transform(data)\n",
    "\n",
    "#Get several search params and learning decays for the LDA model\n",
    "search_params = {'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .6, .7, .8, .9]}\n",
    "lda = LatentDirichletAllocation(learning_method='online', learning_offset=10.0, random_state=0)\n",
    " \n",
    "#Use GridSearchCV in order to run through to get the best param\n",
    "#according to Log Likelihood and and Model Perplexity\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "model.fit(data_vectorized)\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afff4eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "#LDA model using Food Reviews Dataset\n",
    "#####################################################################################################\n",
    "import pandas as pd\n",
    "import re\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "#Get stopword list from file\n",
    "def stopwords():\n",
    "\tstopwordList =[]\n",
    "\twith open(\"stopwords.txt\",'r') as File:\n",
    "\t\tfor line in File:\n",
    "\t\t\tfor word in line.split():\n",
    "\t\t\t\tstopwordList.append(word.lower())\n",
    "\treturn stopwordList\n",
    "\n",
    "stop_words = stopwords()\n",
    "stop_words.extend(['br'])\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]\n",
    "\n",
    "def remove_wordsgreaterthan(texts,length):\n",
    "     return [[word for word in simple_preprocess(str(doc)) \n",
    "             if len(word)>length] for doc in texts]\n",
    "\n",
    "    \n",
    "data = pd.read_csv('FoodReviews.csv',low_memory = False)# Print head\n",
    "print(data)\n",
    "\n",
    "# Remove all columns exclusing the text\n",
    "data.filter(items=['Text']) #'Id','Summary'\n",
    "\n",
    "#Preprocess data\n",
    "# Convert the text to lowercase and get rid of nonalphanumerics and get rid of numbers\n",
    "data['Text'] = data['Text'].map(lambda x: re.sub('[,.!?@$-<>]', '', x))\n",
    "data['Text'] = data['Text'].map(lambda x: x.lower())\n",
    "\n",
    "#filter rows based on length of the review and then choose a sample size\n",
    "data = data.loc[data['Text'].str.len() > 60].sample(1000)\n",
    "data.filter('Text')\n",
    "print(data['Text'])\n",
    "\n",
    "data = data.Text.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "data_words = remove_stopwords(data_words)\n",
    "data_words = remove_wordsgreaterthan(data_words,2)\n",
    "\n",
    "#Using Gensim LDA Model\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "texts = data_words\n",
    "corpus = [id2word.doc2bow(text) for text in texts]# View\n",
    "print(corpus[:1][0][:30])\n",
    "\n",
    "from pprint import pprint\n",
    "num_topics = 10\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8421e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphical Representation of LDA using Food Reviews dataset\n",
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "p = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e8c091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA model evaluation using Food Reviews dataset\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#functions\n",
    "def stopwords():\n",
    "\tstopwordList =[]\n",
    "\twith open(\"stopwords.txt\",'r') as File:\n",
    "\t\tfor line in File:\n",
    "\t\t\tfor word in line.split():\n",
    "\t\t\t\tstopwordList.append(word.lower())\n",
    "\t\t\n",
    "\treturn stopwordList\n",
    "\t\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc))\n",
    "             if word not in stop_words] for doc in texts]\n",
    "             \n",
    "#Tokenize\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "#Data\n",
    "data = pd.read_csv('FoodReviews.csv',low_memory = False)# Print head\n",
    "stop_words = stopwords()\n",
    "\n",
    "#Clean Data\n",
    "data = data.loc[data['Text'].str.len() > 60]\n",
    "data['Text'] = data['Text'].map(lambda x: re.sub('[,.!?@$-<>]', '', x)) #Remove misc\n",
    "data['Text'] = data['Text'].map(lambda x: x.lower()) #Lowercase string\n",
    "data['Text']= data['Text'].map(lambda x: re.sub(r'\\s+', ' ', x))  #Remove double spaces/new lines\n",
    "\n",
    "#Turn to List and remove stop words\n",
    "data.Text.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "data_words = remove_stopwords(data_words)\n",
    "\n",
    "#Vectorize data\n",
    "vectorizer = CountVectorizer(analyzer='word',lowercase=True)\n",
    "data_vectorized = vectorizer.fit_transform(data)\n",
    "\n",
    "#Get several search params and learning decays for the LDA model\n",
    "search_params = {'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .6, .7, .8, .9]}\n",
    "lda = LatentDirichletAllocation(learning_method='online', learning_offset=10.0, random_state=0)\n",
    " \n",
    "#Use GridSearchCV in order to run through to get the best param\n",
    "#according to Log Likelihood and and Model Perplexity\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "model.fit(data_vectorized)\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8134c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "# Non Negative Matrix Factorization Using Datasets Amazon Product Reviews and Food Reviews\n",
    "#####################################################################################################\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BASE_PATH = \"archive\"\n",
    "DATASET_TYPE = \"reviews\"\n",
    "NUM_TOPICS = 9\n",
    "\n",
    "def get_files_to_read(directory_name):\n",
    "\n",
    "    file_list = os.listdir(directory_name)\n",
    "    return file_list\n",
    "\n",
    "def read_file(base_path, dataset_type):\n",
    "    \"\"\"Returns df of specified review text from dataset\n",
    "\n",
    "    Args:\n",
    "        base_path (str): Base directory of file, ex: \"archive\"\n",
    "        file_name (str): File name: \"food.csv\" or \"reviews.csv\"\n",
    "        dataset_type (str): Specifies either food or amazon products\n",
    "\n",
    "    Returns:\n",
    "        [pandas df]: Dataframe of review text\n",
    "    \"\"\"      \n",
    "\n",
    "    if dataset_type == \"food\":\n",
    "        file_path = os.path.join(base_path, \"food.csv\")\n",
    "        file_data = pd.read_csv(file_path)  \n",
    "        review_data = file_data[['Text']].copy()\n",
    "    else:\n",
    "        file_path = os.path.join(base_path, \"reviews.csv\")\n",
    "        file_data = pd.read_csv(file_path)  \n",
    "        review_data = file_data[['reviews.text']].copy()    \n",
    "    \n",
    "    # add .sample(100) for testing\n",
    "    print(review_data.head())\n",
    "\n",
    "    return review_data\n",
    "\n",
    "def prepare_text_regex(text_df, dataset_type):\n",
    "    \"\"\"Taxes text and applies regex filtering for words\n",
    "\n",
    "    Args:\n",
    "        text_df (dataframe): The dataframe of review text\n",
    "        dataset_type (str): The type of dataset: \"food\" or \"product\"\n",
    "\n",
    "    Returns:\n",
    "        text_df (dataframe): Filtered email dataframe after regex\n",
    "    \"\"\"\n",
    "\n",
    "    if dataset_type == \"food\":\n",
    "        column_name = \"Text\"\n",
    "    else:\n",
    "        column_name = \"reviews.text\"\n",
    "\n",
    "    text_df[column_name] = \\\n",
    "    text_df[column_name].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "    text_df[column_name] = \\\n",
    "    text_df[column_name].map(lambda x: re.sub('www', '', x))\n",
    "    text_df[column_name] = \\\n",
    "    text_df[column_name].map(lambda x: x.lower())\n",
    "    text_df[column_name].head()\n",
    "\n",
    "    return text_df\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "def remove_stopwords(texts, stopwords):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "            if word not in stopwords] for doc in texts]\n",
    "\n",
    "def get_NMF_topics(model, vectorizer, top_word_num, num_topics):\n",
    "    \"\"\"Processes data using NMF model\n",
    "\n",
    "    Args:\n",
    "        model (NMF): NMF model class\n",
    "        vectorizer (Vectorizer class): sklearn Vectorizer class\n",
    "        top_word_num (int): Number of words to get for each topic\n",
    "        num_topics (int): Number of topics to look for in dataset\n",
    "\n",
    "    Returns:\n",
    "        nmf_df (dataframe): Pandas dataframe containing topics and top words\n",
    "    \"\"\"\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    top_words_dict = {}\n",
    "    for i in range(num_topics):\n",
    "        word_ids = model.components_[i].argsort()[:-top_word_num - 1:-1]\n",
    "        words = [feature_names[key] for key in word_ids]\n",
    "        words = [re.sub('\\S*@\\S*\\s?', '', word) for word in words]\n",
    "        words = [re.sub('\\s+', ' ', word) for word in words]\n",
    "        words = [re.sub(\"\\'\", \"\", word) for word in words]\n",
    "        top_words_dict[f'Topic #{i+1}'] = words\n",
    "\n",
    "    nmf_df = pd.DataFrame(top_words_dict)\n",
    "    nmf_df.to_csv(f\"nmf_{DATASET_TYPE}_topics.csv\")\n",
    "    \n",
    "    return nmf_df\n",
    "\n",
    "def get_nmf_weights_data(weights, features):\n",
    "    features = np.array(features)\n",
    "    sorted_indices = np.array([list(row[::-1]) for row in np.argsort(np.abs(weights))])\n",
    "    sorted_weights = np.array([list(wt[index]) for wt, index in zip(weights, sorted_indices)])\n",
    "    sorted_terms = np.array([list(features[row]) for row in sorted_indices])\n",
    "\n",
    "    topics = [np.vstack((terms.T, term_weights.T)).T for terms, term_weights in zip(sorted_terms, sorted_weights)]\n",
    "    # print(\"printing topics\")\n",
    "    # print(topics)\n",
    "    return topics\n",
    "\n",
    "def plot_words(model, feature_names, dataset_type, num_top_words=10):\n",
    "    \"\"\"Used to generate the topic plot after NMF processing\n",
    "\n",
    "    Args:\n",
    "        model (NMF): Scikit learn NMF model\n",
    "        feature_names : The features generated from the NMF model\n",
    "        num_top_words : Amount of top words for each topic (used 10 as default)\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(3,3, figsize=(20,15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for topic_index, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[:-num_top_words - 1:-1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_index]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f'Topic #{topic_index +1}',\n",
    "                     fontdict={'fontsize': 25})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        for i in 'top right left'.split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        if dataset_type == \"food\":\n",
    "            fig.suptitle(\"NMF Model Topics -- Food Dataset\", fontsize=30)\n",
    "        else:\n",
    "            fig.suptitle(\"NMF Model Topics -- Product Review Dataset\", fontsize=30)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    # plt.show()\n",
    "\n",
    "    plt.savefig(f\"nmf_model_{dataset_type}_topics.png\")\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # text_files = get_files_to_read(BASE_PATH)\n",
    "\n",
    "    df = read_file(BASE_PATH, DATASET_TYPE)\n",
    "    processed_df = prepare_text_regex(df, DATASET_TYPE)\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'br'])\n",
    "\n",
    "    # This is to differentiate the two datasets: amazon products and food reviews\n",
    "    # since they have different column names for the text data\n",
    "    if DATASET_TYPE == \"food\":\n",
    "        data = processed_df[\"Text\"].values.tolist()\n",
    "    else:\n",
    "        data = processed_df[\"reviews.text\"].values.tolist()\n",
    "\n",
    "    data_words = list(sent_to_words(data))# remove stop words\n",
    "    data_words = remove_stopwords(data_words, stop_words)\n",
    "    # print(\"printing datawords\")\n",
    "    # print(data_words[0])\n",
    "\n",
    "    import gensim.corpora as corpora# Create Dictionary\n",
    "    id2word = corpora.Dictionary(data_words)# Create Corpus\n",
    "    # print(\"printing id2word\")\n",
    "    # print(id2word)\n",
    "    texts = data_words\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "    # Can use this to test word count!\n",
    "    # for i in range(len(corpus[0])):\n",
    "    #     print(f\"Word {corpus[0][i][0]} [{id2word[corpus[0][i][0]]}] -- count: {corpus[0][i][1]} times\")\n",
    "\n",
    "\n",
    "    sentences = [' '.join(text) for text in data_words]\n",
    "    # print(\"printing articles sentences\")\n",
    "    # print(sentences)\n",
    "\n",
    "    # vectorizer = CountVectorizer(analyzer='word', max_features=2000)\n",
    "    # x = vectorizer.fit_transform(sentences)\n",
    "\n",
    "    # transformer = TfidfTransformer()\n",
    "    # x_tfid = transformer.fit_transform(x)\n",
    "\n",
    "    # x_tfid_norm = normalize(x_tfid, norm='l1', axis=1)\n",
    "\n",
    "    tf_vectorizer = TfidfVectorizer(analyzer=\"word\", max_features=2000, stop_words=\"english\")\n",
    "    x = tf_vectorizer.fit_transform(sentences)\n",
    "\n",
    "    num_topics = NUM_TOPICS\n",
    "    nmf_model = NMF(n_components=num_topics, init=\"nndsvd\")\n",
    "    nmf_model.fit(x)\n",
    "\n",
    "    nmf_features = tf_vectorizer.get_feature_names()\n",
    "    nmf_weights = nmf_model.components_\n",
    "    get_NMF_topics(nmf_model, tf_vectorizer, 10, 9)\n",
    "    plot_words(nmf_model, nmf_features, DATASET_TYPE, 10)\n",
    "\n",
    "    # print(\"printing _weights\")\n",
    "    # print(nmf_model.components_)\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
