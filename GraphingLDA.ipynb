{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f96bf10f-286d-4266-90c7-fccb663e4836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Id   ProductId          UserId                      ProfileName  \\\n",
      "0            1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
      "1            2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
      "2            3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
      "3            4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
      "4            5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
      "...        ...         ...             ...                              ...   \n",
      "568449  568450  B001EO7N10  A28KG5XORO54AY                 Lettie D. Carter   \n",
      "568450  568451  B003S1WTCU  A3I8AFVPEE8KI5                        R. Sawyer   \n",
      "568451  568452  B004I613EE  A121AA1GQV751Z                    pksd \"pk_007\"   \n",
      "568452  568453  B004I613EE   A3IBEVCTXKNOH          Kathy A. Welch \"katwel\"   \n",
      "568453  568454  B001LR2CU2  A3LGQPJCZVL9UC                         srfell17   \n",
      "\n",
      "        HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
      "0                          1                       1      5  1303862400   \n",
      "1                          0                       0      1  1346976000   \n",
      "2                          1                       1      4  1219017600   \n",
      "3                          3                       3      2  1307923200   \n",
      "4                          0                       0      5  1350777600   \n",
      "...                      ...                     ...    ...         ...   \n",
      "568449                     0                       0      5  1299628800   \n",
      "568450                     0                       0      2  1331251200   \n",
      "568451                     2                       2      5  1329782400   \n",
      "568452                     1                       1      5  1331596800   \n",
      "568453                     0                       0      5  1338422400   \n",
      "\n",
      "                                   Summary  \\\n",
      "0                    Good Quality Dog Food   \n",
      "1                        Not as Advertised   \n",
      "2                    \"Delight\" says it all   \n",
      "3                           Cough Medicine   \n",
      "4                              Great taffy   \n",
      "...                                    ...   \n",
      "568449                 Will not do without   \n",
      "568450                        disappointed   \n",
      "568451            Perfect for our maltipoo   \n",
      "568452  Favorite Training and reward treat   \n",
      "568453                         Great Honey   \n",
      "\n",
      "                                                     Text  \n",
      "0       I have bought several of the Vitality canned d...  \n",
      "1       Product arrived labeled as Jumbo Salted Peanut...  \n",
      "2       This is a confection that has been around a fe...  \n",
      "3       If you are looking for the secret ingredient i...  \n",
      "4       Great taffy at a great price.  There was a wid...  \n",
      "...                                                   ...  \n",
      "568449  Great for sesame chicken..this is a good if no...  \n",
      "568450  I'm disappointed with the flavor. The chocolat...  \n",
      "568451  These stars are small, so you can give 10-15 o...  \n",
      "568452  These are the BEST treats for training and rew...  \n",
      "568453  I am very satisfied ,product is as advertised,...  \n",
      "\n",
      "[568454 rows x 10 columns]\n",
      "71350     so i bought this product to test it out becaus...\n",
      "421364    thank you amazon and mushroom houseweve been l...\n",
      "429454    i love this chocolate i found it once and cann...\n",
      "61770     we bought this with the hope of preventing cat...\n",
      "398353    when my cousin walked up behind me to stick he...\n",
      "                                ...                        \n",
      "204721    havent gotten my shipment yet from amazon but ...\n",
      "498005    this tea is wonderful im so glad i received it...\n",
      "220699    i cant find this in stores glad to have it on ...\n",
      "296820    this candy is great  i will definitely purchas...\n",
      "469747    i seenthis item on amazon and ordered it  on a...\n",
      "Name: Text, Length: 1000, dtype: object\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 2), (10, 1), (11, 7), (12, 1), (13, 1), (14, 1), (15, 2), (16, 1), (17, 1), (18, 3), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 7), (27, 2), (28, 1), (29, 1)]\n",
      "[(0,\n",
      "  '0.011*\"like\" + 0.010*\"good\" + 0.009*\"great\" + 0.008*\"flavor\" + '\n",
      "  '0.008*\"coffee\" + 0.007*\"love\" + 0.006*\"can\" + 0.006*\"food\" + 0.005*\"taste\" '\n",
      "  '+ 0.005*\"tried\"'),\n",
      " (1,\n",
      "  '0.016*\"taste\" + 0.009*\"like\" + 0.007*\"good\" + 0.006*\"one\" + 0.006*\"flavor\" '\n",
      "  '+ 0.005*\"love\" + 0.005*\"can\" + 0.005*\"bag\" + 0.005*\"just\" + 0.004*\"great\"'),\n",
      " (2,\n",
      "  '0.010*\"like\" + 0.008*\"one\" + 0.007*\"love\" + 0.006*\"just\" + 0.006*\"product\" '\n",
      "  '+ 0.005*\"can\" + 0.005*\"taste\" + 0.004*\"coffee\" + 0.004*\"dont\" + '\n",
      "  '0.004*\"much\"'),\n",
      " (3,\n",
      "  '0.012*\"like\" + 0.008*\"good\" + 0.007*\"great\" + 0.007*\"can\" + 0.006*\"tea\" + '\n",
      "  '0.006*\"will\" + 0.006*\"just\" + 0.005*\"product\" + 0.005*\"food\" + 0.005*\"box\"'),\n",
      " (4,\n",
      "  '0.011*\"coffee\" + 0.011*\"product\" + 0.010*\"taste\" + 0.009*\"good\" + '\n",
      "  '0.009*\"will\" + 0.009*\"like\" + 0.008*\"one\" + 0.007*\"tea\" + 0.006*\"much\" + '\n",
      "  '0.006*\"really\"'),\n",
      " (5,\n",
      "  '0.011*\"like\" + 0.010*\"good\" + 0.010*\"just\" + 0.007*\"flavor\" + 0.007*\"taste\" '\n",
      "  '+ 0.006*\"really\" + 0.006*\"dont\" + 0.006*\"coffee\" + 0.005*\"little\" + '\n",
      "  '0.005*\"great\"'),\n",
      " (6,\n",
      "  '0.013*\"one\" + 0.009*\"just\" + 0.009*\"good\" + 0.008*\"like\" + 0.006*\"get\" + '\n",
      "  '0.006*\"will\" + 0.006*\"coffee\" + 0.006*\"flavor\" + 0.005*\"can\" + '\n",
      "  '0.005*\"love\"'),\n",
      " (7,\n",
      "  '0.011*\"food\" + 0.010*\"tea\" + 0.010*\"like\" + 0.008*\"can\" + 0.007*\"much\" + '\n",
      "  '0.007*\"just\" + 0.007*\"love\" + 0.005*\"better\" + 0.005*\"good\" + '\n",
      "  '0.005*\"really\"'),\n",
      " (8,\n",
      "  '0.008*\"good\" + 0.007*\"will\" + 0.007*\"flavor\" + 0.007*\"just\" + 0.007*\"great\" '\n",
      "  '+ 0.006*\"tea\" + 0.006*\"product\" + 0.005*\"like\" + 0.005*\"really\" + '\n",
      "  '0.005*\"make\"'),\n",
      " (9,\n",
      "  '0.017*\"coffee\" + 0.012*\"like\" + 0.009*\"product\" + 0.008*\"great\" + '\n",
      "  '0.008*\"taste\" + 0.007*\"just\" + 0.007*\"can\" + 0.006*\"food\" + 0.005*\"best\" + '\n",
      "  '0.005*\"one\"')]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "#Get stopword list from file\n",
    "def stopwords():\n",
    "\tstopwordList =[]\n",
    "\twith open(\"stopwords.txt\",'r') as File:\n",
    "\t\tfor line in File:\n",
    "\t\t\tfor word in line.split():\n",
    "\t\t\t\tstopwordList.append(word.lower())\n",
    "\treturn stopwordList\n",
    "\n",
    "stop_words = stopwords()\n",
    "stop_words.extend(['br'])\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]\n",
    "\n",
    "def remove_wordsgreaterthan(texts,length):\n",
    "     return [[word for word in simple_preprocess(str(doc)) \n",
    "             if len(word)>length] for doc in texts]\n",
    "\n",
    "    \n",
    "data = pd.read_csv('FoodReviews.csv',low_memory = False)# Print head\n",
    "print(data)\n",
    "\n",
    "# Remove all columns exclusing the text\n",
    "data.filter(items=['Text']) #'Id','Summary'\n",
    "\n",
    "#Preprocess data\n",
    "# Convert the text to lowercase and get rid of nonalphanumerics and get rid of numbers\n",
    "data['Text'] = data['Text'].map(lambda x: re.sub('[,.!?@$-<>]', '', x))\n",
    "data['Text'] = data['Text'].map(lambda x: x.lower())\n",
    "\n",
    "#filter rows based on length of the review and then choose a sample size\n",
    "data = data.loc[data['Text'].str.len() > 60].sample(1000)\n",
    "summary = data['Summary']\n",
    "data.filter('Text')\n",
    "print(data['Text'])\n",
    "\n",
    "data = data.Text.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "data_words = remove_stopwords(data_words)\n",
    "data_words = remove_wordsgreaterthan(data_words,2)\n",
    "\n",
    "#Using Gensim LDA Model\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "texts = data_words\n",
    "corpus = [id2word.doc2bow(text) for text in texts]# View\n",
    "print(corpus[:1][0][:30])\n",
    "\n",
    "from pprint import pprint\n",
    "num_topics = 10\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af15da88-58ab-4580-a1dd-9d2f27d9fe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd946f8-404d-4ed9-82e5-c1d3ca4051f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "p = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
